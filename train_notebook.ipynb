{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054395a9",
   "metadata": {},
   "source": [
    "<h1 align=center> ğŸ“¹ End-to-End Video Emotion Recognition using MediaPipe and Pre-trained CNNs ğŸ¤– </h1>\n",
    "\n",
    "This project details a complete system for recognizing human emotions from short video clips, designed for a journaling application ğŸ““ that uses video as a cover for entries.\n",
    "\n",
    "A key innovation of this notebook is its unique approach to feature engineering. Instead of analyzing landmark data, this system processes videos by:\n",
    "1.  Using **MediaPipe** ğŸŒ to detect and crop the face in each frame.\n",
    "2.  Sampling these cropped face images at regular intervals throughout a video clip.\n",
    "3.  Stacking the sequence of grayscale images into a single multi-channel tensor.\n",
    "\n",
    "This tensor, which captures both spatial and temporal information, is then fed into a powerful, pre-trained **2D Convolutional Neural Network (CNN)** ğŸ§ , such as EfficientNetV2 or MobileNetV3, for the final emotion classification. ğŸ˜Š\n",
    "\n",
    "The resulting five emotion labels used for training are: \n",
    "- Anger\n",
    "- Happy \n",
    "- Shock\n",
    "- Neutral\n",
    "- Sad\n",
    "\n",
    "## ğŸ’¡ Use Case Possibilities\n",
    "\n",
    "While this model was initially designed for a journaling app, the core technology can be adapted for a wide range of powerful applications:\n",
    "\n",
    "* **ğŸ““ Enhanced Digital Journaling & Mental Wellness:** The primary use case. The app can track mood patterns over time, helping users gain insight into their emotional wellbeing. It could suggest activities or resources based on detected emotional trends.\n",
    "\n",
    "* **ğŸ§  User Experience (UX) Research:** Companies can analyze user reactions to new software, websites, or products in real-time. This provides authentic, unbiased feedback on whether a feature is delightful, confusing, or frustrating.\n",
    "\n",
    "* **ğŸ“š Adaptive E-Learning Platforms:** An online learning system could gauge a student's emotional state. If the system detects confusion or frustration, it could automatically offer hints, supplementary materials, or a different teaching approach.\n",
    "\n",
    "* **ğŸ¬ Audience Reaction Analysis:** Media companies could use this to analyze audience reactions during movie screenings or trailer tests to gauge emotional engagement with the content.\n",
    "\n",
    "* **ğŸš— Driver Monitoring Systems:** In-car cameras could use the model to detect driver states like drowsiness, distraction, or road rage, triggering safety alerts to prevent accidents.\n",
    "\n",
    "<video width=\"600\" controls>\n",
    "  <source src=\"./Assets/journaling_illustration.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\n",
    "*The illustration is generated by AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b947d2",
   "metadata": {},
   "source": [
    "## âš™ï¸ 1. Project Setup and Data Acquisition\n",
    "\n",
    "This initial section handles all the preliminary setup required to get the project running. It begins by importing the essential Python libraries that will be used throughout the notebook, including:\n",
    "- **PyTorch & Torchvision** for building and training the neural networks ğŸ§ .\n",
    "- **OpenCV & MediaPipe** for video processing and facial landmark detection ğŸ“¹.\n",
    "- **Numpy & Pandas** for numerical operations and data handling ğŸ“Š.\n",
    "- **Scikit-learn** for performance evaluation metrics ğŸ“ˆ.\n",
    "- **Matplotlib & Seaborn** for data visualization ğŸ¨.\n",
    "\n",
    "Following the imports, the script automatically downloads the video dataset, which is stored as a ZIP file on Google Drive. It then extracts the contents into a local `./Dataset/Scrapping` directory and cleans up the downloaded ZIP file, ensuring the data is ready for the next stage of processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import time\n",
    "import csv\n",
    "import gc\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torchinfo import summary\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import gdown\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff71a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_download_id = os.getenv('DATASET_DOWNLOAD_ID')\n",
    "zip_path = os.path.join(os.getcwd(), 'Dataset', 'Scrapping.zip')\n",
    "destination_path = os.path.join(os.getcwd(), 'Dataset', 'Scrapping')\n",
    "\n",
    "# Download from Google Drive\n",
    "os.makedirs(destination_path, exist_ok=True)\n",
    "gdown.download(id=dataset_download_id, output=zip_path, quiet=False)\n",
    "\n",
    "# Extract the downloaded ZIP file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(destination_path)\n",
    "\n",
    "# Remove downloaded ZIP\n",
    "os.remove(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cb1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "VIDEO_SOURCE_DIR = os.path.join(os.getcwd(), 'Dataset', 'Scrapping')\n",
    "\n",
    "for dir_path in [VIDEO_SOURCE_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "# Feature Export Parameter\n",
    "SEQUENCE_LENGTH = 90\n",
    "MIN_SEQUENCE_LENGTH = 75\n",
    "FACE_IMG_SIZE = (224, 224)\n",
    "IMG_CAPTURE_INTERVAL = 5\n",
    "VALIDATION_IDS = ['0016', '0017', '0018', '0019', '0020'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aeaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_bbox(frame_shape, landmarks, margin=0.1):\n",
    "    \"\"\"Calculate bounding box from landmarks with a margin.\"\"\"\n",
    "    h, w, _ = frame_shape\n",
    "    x_coords = [lm.x * w for lm in landmarks]\n",
    "    y_coords = [lm.y * h for lm in landmarks]\n",
    "    \n",
    "    x_min, x_max = min(x_coords), max(x_coords)\n",
    "    y_min, y_max = min(y_coords), max(y_coords)\n",
    "    \n",
    "    x_margin = (x_max - x_min) * margin\n",
    "    y_margin = (y_max - y_min) * margin\n",
    "    \n",
    "    x_min = int(max(0, x_min - x_margin))\n",
    "    x_max = int(min(w, x_max + x_margin))\n",
    "    y_min = int(max(0, y_min - y_margin))\n",
    "    y_max = int(min(h, y_max + y_margin))\n",
    "    \n",
    "    return x_min, y_min, x_max, y_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47cfb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_video_chunk(output_path, frame_chunk, frame_size, fps):\n",
    "    \"\"\"\n",
    "    Writes a list of frames to an MP4 video file.\n",
    "    \"\"\"\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video_writer = cv2.VideoWriter(output_path, fourcc, fps, frame_size)\n",
    "    for frame in frame_chunk:\n",
    "        video_writer.write(frame)\n",
    "    video_writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_landmarks(landmarks_flat):\n",
    "    \"\"\"\n",
    "    Normalizes facial landmarks for a single frame to be scale and position invariant.\n",
    "    \"\"\"\n",
    "    \n",
    "    landmarks = np.array(landmarks_flat).reshape(-1, 3)\n",
    "    \n",
    "    x_coords = landmarks[:, 0]\n",
    "    y_coords = landmarks[:, 1]\n",
    "    z_coords = landmarks[:, 2]\n",
    "    \n",
    "    x_min, x_max = np.min(x_coords), np.max(x_coords)\n",
    "    y_min, y_max = np.min(y_coords), np.max(y_coords)\n",
    "    \n",
    "    if x_max - x_min == 0 or y_max - y_min == 0:\n",
    "        return landmarks_flat\n",
    "        \n",
    "    x_normalized = (x_coords - x_min) / (x_max - x_min)\n",
    "    y_normalized = (y_coords - y_min) / (y_max - y_min)\n",
    "    \n",
    "    z_mean = np.mean(z_coords)\n",
    "    z_normalized = z_coords - z_mean\n",
    "    \n",
    "    normalized_landmarks_flat = np.stack([x_normalized, y_normalized, z_normalized], axis=1).flatten().tolist()\n",
    "    \n",
    "    return normalized_landmarks_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557b1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_video_processing(video_path, label):\n",
    "    \"\"\"\n",
    "    Initializes paths, directories, and video capture for processing.\n",
    "    Determines if the video is for training or validation based on VALIDATION_IDS.\n",
    "    \"\"\"\n",
    "    \n",
    "    base_filename = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    \n",
    "    # Determine if the file is for training or validation\n",
    "    video_id = base_filename.split('_')[1]\n",
    "    split_folder = 'val' if video_id in VALIDATION_IDS else 'train'\n",
    "\n",
    "    # Define output directories with the new structure\n",
    "    output_dirs = {\n",
    "        'csv': os.path.join(os.getcwd(), 'Data', 'Scrapping', split_folder, 'FaceMesh', label),\n",
    "        'tiff': os.path.join(os.getcwd(), 'Data', 'Scrapping', split_folder, 'FaceImage', label),\n",
    "        'video': os.path.join(os.getcwd(), 'Data', 'Scrapping', split_folder, 'Video', label)\n",
    "    }\n",
    "\n",
    "    for d in output_dirs.values():\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return None\n",
    "\n",
    "    video_properties = {\n",
    "        'fps': cap.get(cv2.CAP_PROP_FPS),\n",
    "        'frame_size': (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    }\n",
    "    \n",
    "    return cap, base_filename, output_dirs, video_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame_data(face_mesh, frame, last_known_landmarks):\n",
    "    \"\"\"\n",
    "    Processes a single frame to extract facial landmarks.\n",
    "    \"\"\"\n",
    "    \n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb_frame)\n",
    "    \n",
    "    landmarks_present = False\n",
    "    if results.multi_face_landmarks:\n",
    "        face_landmarks = results.multi_face_landmarks[0]\n",
    "        current_landmarks_flat = [c for lm in face_landmarks.landmark for c in (lm.x, lm.y, lm.z)]\n",
    "        landmarks_present = True\n",
    "    else:\n",
    "        face_landmarks = None\n",
    "        current_landmarks_flat = last_known_landmarks\n",
    "\n",
    "    normalized_landmarks = normalize_landmarks(current_landmarks_flat)\n",
    "    return normalized_landmarks, face_landmarks, current_landmarks_flat, landmarks_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad306a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_chunk(base_filename, chunk_idx, output_dirs, data_chunks, video_properties):\n",
    "    \"\"\"\n",
    "    Saves the collected data chunks to files.\n",
    "    \"\"\"\n",
    "    \n",
    "    chunk_filename_base = f\"{base_filename}_{chunk_idx:04d}\"\n",
    "    \n",
    "    # Save CSV\n",
    "    csv_path = os.path.join(output_dirs['csv'], f\"{chunk_filename_base}.csv\")\n",
    "    header = ['frame'] + [f'p_{i}' for i in range(468 * 3)]\n",
    "    with open(csv_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(data_chunks['landmarks'])\n",
    "\n",
    "    # Save TIFF\n",
    "    if data_chunks['images']:\n",
    "        tiff_path = os.path.join(output_dirs['tiff'], f\"{chunk_filename_base}.tiff\")\n",
    "        images_array = np.array(data_chunks['images'])\n",
    "        images_with_channel = np.expand_dims(images_array, axis=0) # Shape: (1, D, H, W)\n",
    "        tiff.imwrite(tiff_path, images_with_channel, imagej=True)\n",
    "\n",
    "    # Save Video\n",
    "    video_path_out = os.path.join(output_dirs['video'], f\"{chunk_filename_base}.mp4\")\n",
    "    write_video_chunk(video_path_out, data_chunks['frames'], video_properties['frame_size'], video_properties['fps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9febe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, label):\n",
    "    \"\"\"\n",
    "    Video feature extraction pipeline.\n",
    "    \"\"\"\n",
    "    \n",
    "    setup_info = setup_video_processing(video_path, label)\n",
    "    if not setup_info:\n",
    "        return\n",
    "    cap, base_filename, output_dirs, video_properties = setup_info\n",
    "\n",
    "    frame_idx, chunk_idx = 0, 1\n",
    "    landmark_chunk, image_chunk, frame_chunk = [], [], []\n",
    "    last_known_landmarks = [0] * (468 * 3)\n",
    "    last_known_image = np.zeros(FACE_IMG_SIZE, dtype=np.uint8)\n",
    "\n",
    "    mp_face_mesh = mp.solutions.face_mesh\n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, refine_landmarks=False, min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_chunk.append(frame)\n",
    "            \n",
    "            normalized_landmarks, face_landmarks_obj, current_landmarks, landmarks_found = process_frame_data(face_mesh, frame, last_known_landmarks)\n",
    "            last_known_landmarks = current_landmarks\n",
    "            \n",
    "            if frame_idx % IMG_CAPTURE_INTERVAL == 0:\n",
    "                if landmarks_found:\n",
    "                    x_min, y_min, x_max, y_max = get_face_bbox(frame.shape, face_landmarks_obj.landmark)\n",
    "                    face_crop = frame[y_min:y_max, x_min:x_max]\n",
    "                    if face_crop.size > 0:\n",
    "                        gray_face = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "                        resized_face = cv2.resize(gray_face, FACE_IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
    "                        last_known_image = resized_face # Update the last known image\n",
    "                        image_chunk.append(last_known_image)\n",
    "                    else:\n",
    "                        # If crop is empty, use the last known good image\n",
    "                        image_chunk.append(last_known_image)\n",
    "                else:\n",
    "                    # If no landmarks are found, use the last known good image\n",
    "                    image_chunk.append(last_known_image)\n",
    "\n",
    "            landmark_chunk.append([frame_idx] + normalized_landmarks)\n",
    "            frame_idx += 1\n",
    "\n",
    "            if len(landmark_chunk) == SEQUENCE_LENGTH:\n",
    "                data_chunks = {'landmarks': landmark_chunk, 'images': image_chunk, 'frames': frame_chunk}\n",
    "                save_data_chunk(base_filename, chunk_idx, output_dirs, data_chunks, video_properties)\n",
    "                \n",
    "                chunk_idx += 1\n",
    "                landmark_chunk, image_chunk, frame_chunk = [], [], []\n",
    "                # Reset last_known_image for the new chunk to avoid carry-over\n",
    "                last_known_image = np.zeros(FACE_IMG_SIZE, dtype=np.uint8)\n",
    "\n",
    "    if len(landmark_chunk) >= MIN_SEQUENCE_LENGTH:\n",
    "        last_landmark_row = landmark_chunk[-1]\n",
    "        last_frame = frame_chunk[-1]\n",
    "        \n",
    "        # Pad the image chunk to its expected size using the last known image\n",
    "        num_images_expected = -(-SEQUENCE_LENGTH // IMG_CAPTURE_INTERVAL) # Ceiling division\n",
    "        while len(image_chunk) < num_images_expected:\n",
    "            image_chunk.append(last_known_image)\n",
    "\n",
    "        # Pad the landmark and frame chunks to the full sequence length\n",
    "        while len(landmark_chunk) < SEQUENCE_LENGTH:\n",
    "            landmark_chunk.append(last_landmark_row)\n",
    "            frame_chunk.append(last_frame)\n",
    "        \n",
    "        data_chunks = {'landmarks': landmark_chunk, 'images': image_chunk, 'frames': frame_chunk}\n",
    "        save_data_chunk(base_filename, chunk_idx, output_dirs, data_chunks, video_properties)\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8214719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_task(args):\n",
    "    filename, label = args\n",
    "    video_path = os.path.join(VIDEO_SOURCE_DIR, filename)\n",
    "    try:\n",
    "        process_video(video_path, label)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process {video_path}. Error: {e}\")\n",
    "\n",
    "all_tasks = []\n",
    "for filename in os.listdir(VIDEO_SOURCE_DIR):\n",
    "    if filename.endswith('.mp4'):\n",
    "        try:\n",
    "            label = filename.split('_')[0] \n",
    "            all_tasks.append((filename, label))\n",
    "        except IndexError:\n",
    "            print(f\"Skipping file with unexpected format: {filename}\")\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "    list(tqdm(executor.map(worker_task, all_tasks), total=len(all_tasks), desc=\"Processing Videos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21796a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(os.getcwd(), 'Data', 'Scrapping')\n",
    "BATCH_SIZE = 24\n",
    "\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
    "VAL_DIR = os.path.join(DATA_DIR, 'val')\n",
    "\n",
    "for dir_path in [TRAIN_DIR, VAL_DIR]:\n",
    "    os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a6475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that loads only the TIFF image sequences and their corresponding labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_file_paths, labels, label_map, is_train=False):\n",
    "        super().__init__()\n",
    "        self.image_file_paths = image_file_paths\n",
    "        self.str_labels = labels\n",
    "        self.label_map = label_map\n",
    "        self.is_train = is_train\n",
    "\n",
    "        # Define the image augmentation pipeline, only used if is_train is True\n",
    "        if self.is_train:\n",
    "            self.image_augment = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.RandomAffine(degrees=30, translate=(0.2, 0.2), scale=(0.75, 1.05)),\n",
    "                T.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.0)),\n",
    "                T.RandomErasing(p=0.2, scale=(0.02, 0.1), ratio=(0.3, 3.3), value=0),\n",
    "            ])\n",
    "\n",
    "        self.image_tensors = []\n",
    "        self.label_tensors = []\n",
    "\n",
    "        def _load_file(args):\n",
    "            image_path, label_str = args\n",
    "            try:\n",
    "                # Load image data (TIFF), normalize to [0, 1], and squeeze the first dimension\n",
    "                image_data = torch.tensor(tiff.imread(image_path), dtype=torch.float32).squeeze(0) / 255.0\n",
    "\n",
    "                # Apply augmentation only to the training set\n",
    "                if self.is_train:\n",
    "                    image_data = self.image_augment(image_data)\n",
    "\n",
    "                # Get label\n",
    "                label_int = self.label_map[label_str]\n",
    "                label = torch.tensor(label_int, dtype=torch.long)\n",
    "                \n",
    "                return image_data, label\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping file due to error: {image_path} | Error: {e}\")\n",
    "                return None, None\n",
    "\n",
    "        # Parallelized Data Loading\n",
    "        tasks = zip(self.image_file_paths, self.str_labels)\n",
    "        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "            results = list(tqdm(executor.map(_load_file, tasks), total=len(self.image_file_paths), desc=\"Processing image files\"))\n",
    "\n",
    "        for img_data, lbl in results:\n",
    "            if all(x is not None for x in [img_data, lbl]):\n",
    "                self.image_tensors.append(img_data)\n",
    "                self.label_tensors.append(lbl)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_tensors[idx], self.label_tensors[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad9f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_files_and_labels(split_dir):\n",
    "    files = []\n",
    "    labels = []\n",
    "    faceimage_dir = os.path.join(split_dir, 'FaceImage')\n",
    "    if not os.path.isdir(faceimage_dir):\n",
    "        return [], []\n",
    "    \n",
    "    emotion_folders = [d for d in os.listdir(faceimage_dir) if os.path.isdir(os.path.join(faceimage_dir, d))]\n",
    "    for emotion in emotion_folders:\n",
    "        emotion_path = os.path.join(faceimage_dir, emotion)\n",
    "        tiff_files = [os.path.join(emotion_path, f) for f in os.listdir(emotion_path) if f.endswith('.tiff')]\n",
    "        files.extend(tiff_files)\n",
    "        labels.extend([emotion] * len(tiff_files))\n",
    "    return files, labels\n",
    "\n",
    "train_image_files, train_image_labels_str = get_image_files_and_labels(TRAIN_DIR)\n",
    "val_image_files, val_image_labels_str = get_image_files_and_labels(VAL_DIR)\n",
    "\n",
    "emotion_folders = [d for d in os.listdir(os.path.join(TRAIN_DIR, 'FaceImage')) if os.path.isdir(os.path.join(TRAIN_DIR, 'FaceImage', d))]\n",
    "unique_labels = sorted(emotion_folders)\n",
    "label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "num_classes = len(label_map)\n",
    "\n",
    "train_image_dataset = ImageDataset(train_image_files, train_image_labels_str, label_map, is_train=True)\n",
    "val_image_dataset = ImageDataset(val_image_files, val_image_labels_str, label_map, is_train=False)\n",
    "\n",
    "train_image_loader = DataLoader(train_image_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_image_loader = DataLoader(val_image_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747c672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2ModelM(nn.Module):\n",
    "    def __init__(self, num_classes, num_input_channels):\n",
    "        super(EfficientNetV2ModelM, self).__init__()\n",
    "\n",
    "        self.efficientnet = models.efficientnet_v2_m(weights=models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        original_first_layer = self.efficientnet.features[0][0]\n",
    "        # Create a new conv layer with the correct number of input channels\n",
    "        first_layer = nn.Conv2d(\n",
    "            in_channels=num_input_channels,\n",
    "            out_channels=original_first_layer.out_channels,\n",
    "            kernel_size=original_first_layer.kernel_size,\n",
    "            stride=original_first_layer.stride,\n",
    "            padding=original_first_layer.padding,\n",
    "            bias=original_first_layer.bias\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            first_layer.weight.data = original_first_layer.weight.data.mean(dim=1, keepdim=True).repeat(1, num_input_channels, 1, 1)\n",
    "\n",
    "        self.efficientnet.features[0][0] = first_layer\n",
    "\n",
    "        in_features = self.efficientnet.classifier[1].in_features\n",
    "        hidden_dim = 512\n",
    "        \n",
    "        # Dense layers after EfficientNet\n",
    "        self.efficientnet.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.6, inplace=False),\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe85f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2ModelS(nn.Module):\n",
    "    def __init__(self, num_classes, num_input_channels):\n",
    "        super(EfficientNetV2ModelS, self).__init__()\n",
    "\n",
    "        self.efficientnet = models.efficientnet_v2_s(weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        original_first_layer = self.efficientnet.features[0][0]\n",
    "        # Create a new conv layer with the correct number of input channels\n",
    "        first_layer = nn.Conv2d(\n",
    "            in_channels=num_input_channels,\n",
    "            out_channels=original_first_layer.out_channels,\n",
    "            kernel_size=original_first_layer.kernel_size,\n",
    "            stride=original_first_layer.stride,\n",
    "            padding=original_first_layer.padding,\n",
    "            bias=original_first_layer.bias\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            first_layer.weight.data = original_first_layer.weight.data.mean(dim=1, keepdim=True).repeat(1, num_input_channels, 1, 1)\n",
    "            \n",
    "        self.efficientnet.features[0][0] = first_layer\n",
    "        \n",
    "        in_features = self.efficientnet.classifier[1].in_features\n",
    "        hidden_dim = 512\n",
    "        \n",
    "        # Dense layers after EfficientNet\n",
    "        self.efficientnet.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.6, inplace=False),\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.efficientnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3Large(nn.Module):\n",
    "    def __init__(self, num_classes, num_input_channels):\n",
    "        super(MobileNetV3Large, self).__init__()\n",
    "\n",
    "        self.mobilenet = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.IMAGENET1K_V2)\n",
    "\n",
    "        original_first_layer = self.mobilenet.features[0][0]\n",
    "        first_layer = nn.Conv2d(\n",
    "            in_channels=num_input_channels,\n",
    "            out_channels=original_first_layer.out_channels,\n",
    "            kernel_size=original_first_layer.kernel_size,\n",
    "            stride=original_first_layer.stride,\n",
    "            padding=original_first_layer.padding,\n",
    "            bias=original_first_layer.bias\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            first_layer.weight.data = original_first_layer.weight.data.mean(dim=1, keepdim=True).repeat(1, num_input_channels, 1, 1)\n",
    "        self.mobilenet.features[0][0] = first_layer\n",
    "\n",
    "        in_features = self.mobilenet.classifier[0].in_features\n",
    "        hidden_dim = 512\n",
    "        \n",
    "        # Dense layers after MobileNet\n",
    "        self.mobilenet.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.6, inplace=False),\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(p=0.5, inplace=False),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mobilenet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a00802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs,\n",
    "    device,\n",
    "    save_dir,\n",
    "    log_file_name,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=1e-5,\n",
    "    gamma=0.985\n",
    "):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    log_file_path = os.path.join(save_dir, log_file_name)\n",
    "    log_header = ['epoch', 'time_seconds', 'train_acc', 'train_loss', 'val_acc', 'val_loss', 'learning_rate']\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    with open(log_file_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(log_header)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        train_loss, train_correct, train_total = 0, 0, 0\n",
    "\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False)\n",
    "        for image_batch, labels_batch in train_pbar:\n",
    "            image_batch, labels_batch = image_batch.to(device), labels_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(image_batch)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels_batch.size(0)\n",
    "            train_correct += (predicted == labels_batch).sum().item()\n",
    "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{(predicted == labels_batch).sum().item() / labels_batch.size(0):.4f}'})\n",
    "\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for image_batch, labels_batch in val_pbar:\n",
    "                image_batch, labels_batch = image_batch.to(device), labels_batch.to(device)\n",
    "                outputs = model(image_batch)\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels_batch.size(0)\n",
    "                val_correct += (predicted == labels_batch).sum().item()\n",
    "                val_pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{(predicted == labels_batch).sum().item() / labels_batch.size(0):.4f}'})\n",
    "\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        # Logging and Model Saving\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Time: {epoch_duration:.2f}s | \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}% | \"\n",
    "              f\"LR: {current_lr:.6f}\")\n",
    "\n",
    "        with open(log_file_path, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([epoch + 1, epoch_duration, train_acc, avg_train_loss, val_acc, avg_val_loss, current_lr])\n",
    "\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, 'last_model.pth'))\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, 'best_val_acc_model.pth'))\n",
    "            print(f\"ğŸ‰ New best val acc model saved: {val_acc:.2f}%\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, 'best_val_loss_model.pth'))\n",
    "            print(f\"âœ¨ New best val loss model saved: {avg_val_loss:.4f}\")\n",
    "\n",
    "    total_training_time = time.time() - start_time\n",
    "    print(f\"\\n--- Training Finished ---\")\n",
    "    print(f\"Total Training Time: {total_training_time / 60:.2f} minutes\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"Lowest Validation Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    del model\n",
    "    del optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model_class, \n",
    "    model_kwargs, \n",
    "    best_model_path, \n",
    "    val_loader, \n",
    "    device, \n",
    "    label_map\n",
    "):\n",
    "    # Instantiate and load weights\n",
    "    model = model_class(**model_kwargs).to(device)\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.numpy())\n",
    "\n",
    "    # Reverse label_map for readable output\n",
    "    idx_to_label = {v: k for k, v in label_map.items()}\n",
    "    target_names = [idx_to_label[i] for i in range(len(idx_to_label))]\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names))\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d5c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_channels = -(-SEQUENCE_LENGTH // IMG_CAPTURE_INTERVAL)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00029ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetv2s_model = EfficientNetV2ModelS(\n",
    "    num_classes=num_classes,\n",
    "    num_input_channels=image_channels\n",
    ").to(device)\n",
    "\n",
    "print(\"EfficientNetV2-S Summary:\")\n",
    "print(summary(efficientnetv2s_model, input_size=(BATCH_SIZE, image_channels, FACE_IMG_SIZE[0], FACE_IMG_SIZE[1])), '\\n\\n')\n",
    "\n",
    "train_model(\n",
    "    efficientnetv2s_model,\n",
    "    train_image_loader,\n",
    "    val_image_loader,\n",
    "    num_epochs=50,\n",
    "    device=device,\n",
    "    save_dir=os.path.join(os.getcwd(), \"Models\", f\"Models_EfficientNetV2S_{IMG_CAPTURE_INTERVAL}\"),\n",
    "    log_file_name=\"training_logs_efficientnetv2s.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetv2m_model = EfficientNetV2ModelM(\n",
    "    num_classes=num_classes,\n",
    "    num_input_channels=image_channels\n",
    ").to(device)\n",
    "\n",
    "print(\"EfficientNetV2-M Summary:\")\n",
    "print(summary(efficientnetv2m_model, input_size=(BATCH_SIZE, image_channels, FACE_IMG_SIZE[0], FACE_IMG_SIZE[1])), '\\n\\n')\n",
    "\n",
    "train_model(\n",
    "    efficientnetv2m_model,\n",
    "    train_image_loader,\n",
    "    val_image_loader,\n",
    "    num_epochs=50,\n",
    "    device=device,\n",
    "    save_dir=os.path.join(os.getcwd(), \"Models\", f\"Models_EfficientNetV2M_{IMG_CAPTURE_INTERVAL}\"),\n",
    "    log_file_name=\"training_logs_efficientnetv2m.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69600832",
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetv3large_model = MobileNetV3Large(\n",
    "    num_classes=num_classes,\n",
    "    num_input_channels=image_channels\n",
    ").to(device)\n",
    "\n",
    "print(\"MobileNetV3Large Summary:\")\n",
    "print(summary(mobilenetv3large_model, input_size=(BATCH_SIZE, image_channels, FACE_IMG_SIZE[0], FACE_IMG_SIZE[1])), '\\n\\n')\n",
    "\n",
    "train_model(\n",
    "    mobilenetv3large_model,\n",
    "    train_image_loader,\n",
    "    val_image_loader,\n",
    "    num_epochs=50,\n",
    "    device=device,\n",
    "    save_dir=os.path.join(os.getcwd(), \"Models\", f\"Models_MobileNetV3Large_{IMG_CAPTURE_INTERVAL}\"),\n",
    "    log_file_name=\"training_logs_mobilenetv3large.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd171af",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\n",
    "    EfficientNetV2ModelS,\n",
    "    {\"num_classes\": num_classes, \"num_input_channels\": image_channels},\n",
    "    os.path.join(os.getcwd(), \"Models\", f\"Models_EfficientNetV2S_{IMG_CAPTURE_INTERVAL}\", \"best_val_acc_model.pth\"),\n",
    "    val_image_loader,\n",
    "    device,\n",
    "    label_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ae6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\n",
    "    EfficientNetV2ModelM,\n",
    "    {\"num_classes\": num_classes, \"num_input_channels\": image_channels},\n",
    "    os.path.join(os.getcwd(), \"Models\", f\"Models_EfficientNetV2M_{IMG_CAPTURE_INTERVAL}\", \"best_val_acc_model.pth\"),\n",
    "    val_image_loader,\n",
    "    device,\n",
    "    label_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d39b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\n",
    "    MobileNetV3Large,\n",
    "    {\"num_classes\": num_classes, \"num_input_channels\": image_channels},\n",
    "    os.path.join(os.getcwd(), \"Models\", f\"Models_MobileNetV3Large_{IMG_CAPTURE_INTERVAL}\", \"best_val_acc_model.pth\"),\n",
    "    val_image_loader,\n",
    "    device,\n",
    "    label_map\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
